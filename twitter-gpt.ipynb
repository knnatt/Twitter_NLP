{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gpt labeling don't mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "     ---------------------------------------- 0.0/83.5 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 61.4/83.5 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 83.5/83.5 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\natkn\\anaconda3\\envs\\gpt-twitter\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.16.2-cp38-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\natkn\\anaconda3\\envs\\gpt-twitter\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 226.7/226.7 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.6/85.6 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.8/77.8 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 394.8/394.8 kB 25.6 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.2-cp38-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.7/1.9 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.1 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.6/61.6 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, sniffio, pydantic-core, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 certifi-2024.2.2 distro-1.9.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 idna-3.6 openai-1.12.0 pydantic-2.6.1 pydantic-core-2.16.2 sniffio-1.3.0 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "#client = OpenAI()\n",
    "openai.api_key = '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'\n",
    "\n",
    "examples = [\n",
    "    [\"Department of public works: ถนน ทางเท้าแย่\", \"Thai\"],\n",
    "    [\"Department of public works: ถนน ทางเท้า พัง ชำรุด แตก\", \"Thai\"],\n",
    "    [\"Department of public works: เดินสะดุด ไม่มีทางออก\", \"Thai\"],\n",
    "    [\"Police department: มอไซที่ขับขึ้นฟุตบาท\", \"Thai\"],\n",
    "    [\"Police department: จอดรถขวางทาง\", \"Thai\"],\n",
    "    [\"Municipality: ขายของยึดฟุตบาท\", \"Thai\"],\n",
    "    [\"Municipality: ร้านค้า ขายของบนทางเท้า\", \"Thai\"],\n",
    "    [\"Municipality: เทศกิจ\", \"Thai\"],\n",
    "    [\"Municipality: ขายของบนฟุตบาท\", \"Thai\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a classification request\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Thai Restaurant Review:\\n\" + \"\\n\".join(examples[0]) + \"\\n--\\nร้านอาหารบนทางเท้า\\nSentiment:\",\n",
    "    temperature=0,\n",
    "    max_tokens=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the response\n",
    "label = response['choices'][0]['text'].strip()\n",
    "\n",
    "print(\"Predicted label:\", label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'Classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClassification\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# You can choose other models as well\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     examples\u001b[38;5;241m=\u001b[39mexamples,\n\u001b[0;32m      4\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mร้านอาหารบนทางเท้า\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepartment of public works\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolice department\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMunicipality\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m     language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Classification'"
     ]
    }
   ],
   "source": [
    "response = openai.Classification.create(\n",
    "    model=\"text-davinci-003\",  # You can choose other models as well\n",
    "    examples=examples,\n",
    "    query=\"ร้านอาหารบนทางเท้า\",\n",
    "    labels=[\"Department of public works\", \"Police department\", \"Municipality\"],\n",
    "    language=\"th\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = response['label']\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
    "        \"role\": \"assistant\"\n",
    "      },\n",
    "      \"logprobs\": null\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1677664795,\n",
    "  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 17,\n",
    "    \"prompt_tokens\": 57,\n",
    "    \"total_tokens\": 74\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
