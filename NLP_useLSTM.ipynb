{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supposed the input is in df, the data of X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "#keras embed method, will update with the tokenizer and weight file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file):\n",
    "    with open(file, \"r\") as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"./org_model_weights.h5\")\n",
    "    return model\n",
    "\n",
    "def preproces_df(csv):\n",
    "    #preprocessing the data from df \n",
    "    df_a = pd.read_csv(csv)\n",
    "    df_a['split_text'] = df_a.apply(lambda row: word_tokenize(row['tweet_text'],engine=\"newmm\",keep_whitespace=False), axis=1) #use this\n",
    "    df_a['combined'] = [' '.join(lst) for lst in df_a['split_text']]\n",
    "    df_a['cleaned'] = df_a['combined'].apply(deEmojify)\n",
    "    df_a['cleaned'] = df_a['cleaned'].apply(stopwords_rm) \n",
    "    df_a['split_cleaned'] = df_a.apply(lambda row: word_tokenize(row['cleaned'],engine=\"newmm\",keep_whitespace=False), axis=1)\n",
    "    df_a['word_length'] = df_a['split_cleaned'].str.len()\n",
    "    return df_a\n",
    "\n",
    "#loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#sub function below---------------------------------------------------------------------------------------------------\n",
    "def stopwords_rm(text):\n",
    "    stopwords = set(thai_stopwords())\n",
    "    stopwords.update([\"nan\", \"-\", \"_\", \"\", \" \", \"฿\" ,\"ค่ะ\", \"ครับ\", \"จ้า\"])  # Add more stopwords as needed\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    cleaned_text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    return cleaned_text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "# Load model and weight\n",
    "def Main(df):\n",
    "    model = load_model(\"./org_model_architecture.json\")\n",
    "    df = preproces_df(df)\n",
    "    print('predicting...')\n",
    "    MAX_SEQUENCE_LENGTH = 63 #training file 44\n",
    "    MAX_WORDS = 2500 #2000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "\n",
    "    X_test=[df['cleaned'].iloc[-1]]\n",
    "    X_test=tokenizer.texts_to_sequences(X_test)\n",
    "    X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(X_test)\n",
    "    org = ['public work (โยธา)', 'municipal office (เทศกิจ)' ,'police department']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', org[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking from ori excel\n",
    "    #print('real cat is : ',df['org'].iloc[-1])\n",
    "    if 'org' not in df.index:\n",
    "        df.loc['org'] = 0 \n",
    "\n",
    "    df['org'].iloc[-1] = org[int(predictions.argmax(axis=-1))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def Main_relevant(df):\n",
    "    model = load_model(\"./relevant_model_architecture.json\")\n",
    "    df = preproces_df(df) #df['tweet_text'] เป็นstring ของ tweet \n",
    "    print('predicting...')\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 63 #same as training file\n",
    "    MAX_WORDS = 3000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "\n",
    "    X_test=[df['cleaned'].iloc[-1]]\n",
    "    X_test=tokenizer.texts_to_sequences(X_test)\n",
    "    X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(X_test)\n",
    "    rev = ['not relevant','relevant']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', rev[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking\n",
    "    print('real cat is : ',df['rev'].iloc[-1])\n",
    "\n",
    "    df_drop = df #if not change then it is relevant\n",
    "\n",
    "    if int(predictions.argmax(axis=-1))== 0:\n",
    "        print('not relevant')\n",
    "        df_drop = df[-1] #store the dropped df\n",
    "        df = df[:-1] #drop the last one\n",
    "    return df, df_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "result: [[0.11716956 0.44871703 0.43411338]]\n",
      "classs predicted:  municipal office (เทศกิจ)\n",
      "confidence %:  [0.44871703]\n",
      "real cat is :  nan\n"
     ]
    }
   ],
   "source": [
    "df_updated, df_drop = Main_relevant(\"./TestNLP_labeled_lstm.csv\") #input the df location\n",
    "if df_drop[-1] == df_updated[-1]: #relevant\n",
    "    df = Main(df_updated)\n",
    "    print('relevant, updated')\n",
    "    print('org is : ', )\n",
    "else: \n",
    "    print('irrelevant, row deleted')\n",
    "    print('deleted row : ', df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                              1004\n",
       "username                                                    AitKanphong\n",
       "tweet_id                                            1701505503111819503\n",
       "tweet_text            เหล่าสิงห์มอเตอร์ไซด์สายเท่ทุกท่านครับ กทม จับ...\n",
       "org                                                                 NaN\n",
       "relevant                                                          False\n",
       "category                                                        ทางเท้า\n",
       "query                 (\"ทางเท้า\" OR \"ทางเดิน\" OR \"ฟุตบาท\") AND (\"กรุ...\n",
       "datetime_of_tweet                                    2023-09-12 7:58:19\n",
       "datetime_of_query                                   2023-11-16 14:52:12\n",
       "link                  https://twitter.com/AitKanphong/status/1701505...\n",
       "mentioned_location                                              มีนบุรี\n",
       "location                                                            NaN\n",
       "image                 ['https://pbs.twimg.com/media/F5z1289bgAAzKC0?...\n",
       "split_text            [เหล่า, สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท...\n",
       "combined              เหล่า สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน ครับ...\n",
       "cleaned               สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน กทม อ. เอ ...\n",
       "split_cleaned         [สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท่าน, กท...\n",
       "word_length                                                          38\n",
       "Name: 552, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "X_test=tokenizer.texts_to_sequences(X_test)\n",
    "X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
