{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supposed the input is in df, the data of X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "#keras embed method, will update with the tokenizer and weight file\n",
    "#python3.9 keras3.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pythainlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_stopwords\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_json\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pythainlp'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file):\n",
    "    with open(file, \"r\") as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"./org_model_weights.h5\")\n",
    "    return model\n",
    "\n",
    "def preproces_df(csv):\n",
    "    #preprocessing the data from df \n",
    "    df_a = pd.read_csv(csv)\n",
    "    df_a['split_text'] = df_a.apply(lambda row: word_tokenize(row['tweet_text'],engine=\"newmm\",keep_whitespace=False), axis=1) #use this\n",
    "    df_a['combined'] = [' '.join(lst) for lst in df_a['split_text']]\n",
    "    df_a['cleaned'] = df_a['combined'].apply(deEmojify)\n",
    "    df_a['cleaned'] = df_a['cleaned'].apply(stopwords_rm) \n",
    "    df_a['cleaned'] = alone(df_a) #new\n",
    "    df_a['split_cleaned'] = df_a.apply(lambda row: word_tokenize(row['cleaned'],engine=\"newmm\",keep_whitespace=False), axis=1)\n",
    "    #df_a['word_length'] = df_a['split_cleaned'].str.len()\n",
    "    df_a = df_a.dropna(subset=['tweet_text']) #new\n",
    "\n",
    "    df_a['contain_area'],df_a['contain_good'] = '2','2' #new -holder new column\n",
    "    for i in range(df_a['split_cleaned'].shape[0]):  #new\n",
    "        if 'ดี' in df_a['split_cleaned'].iloc[i]: \n",
    "            df_a['contain_good'].iloc[i] = '1'\n",
    "        else:\n",
    "            df_a['contain_good'].iloc[i] = '0'\n",
    "\n",
    "    for i in range(df_a['split_cleaned'].shape[0]): #new\n",
    "        if 'พื้นที่' in df_a['split_cleaned'].iloc[i]:\n",
    "            df_a['contain_area'].iloc[i] = '1'\n",
    "        else:\n",
    "            df_a['contain_area'].iloc[i] = '0'\n",
    "    df_a = df_a.dropna(subset=['tweet_text'])\n",
    "\n",
    "\n",
    "    return df_a\n",
    "\n",
    "#loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#sub function below---------------------------------------------------------------------------------------------------\n",
    "def stopwords_rm(text):\n",
    "    stopwords = set(thai_stopwords())\n",
    "    stopwords.update([\"nan\", \"-\", \"_\", \"\", \" \", \"฿\" ,\"ค่ะ\", \"ครับ\", \"จ้า\"])  # Add more stopwords as needed\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    cleaned_text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    return cleaned_text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def alone(df):\n",
    "    print('Are you alone?')\n",
    "    i = 0 #row number\n",
    "    df['cleaned1'] = ''\n",
    "    for row in df['cleaned']:\n",
    "        words = row.split() #j is array of string\n",
    "        cleaned = [word for word in words if len(word) > 1]\n",
    "        cleaned_string = \" \".join(cleaned)\n",
    "        df['cleaned1'].iloc[i] = cleaned_string\n",
    "        i += 1\n",
    "    print('done cleaning alone')\n",
    "    return df['cleaned1']\n",
    "\n",
    "def pau_anal(df_a): #new\n",
    "    contain_good = df_a['contain_good'].to_numpy()\n",
    "    good_reshaped = np.reshape(contain_good, (contain_good.shape[0], 1))\n",
    "    good_reshaped = [int(x) for n in range(int(df_a.shape[0]))for x in good_reshaped[n][0]] #list in int \n",
    "    np_good = np.array(good_reshaped)\n",
    "    good_reshaped = np.reshape(np_good, (np_good.shape[0], 1))\n",
    "\n",
    "    contain_area = df_a['contain_area'].to_numpy()\n",
    "    area_reshaped = np.reshape(contain_area, (contain_area.shape[0], 1))\n",
    "    area_reshaped = [int(x) for n in range(int(df_a.shape[0]))for x in area_reshaped[n][0]] #list in int \n",
    "    np_area = np.array(area_reshaped)\n",
    "    area_reshaped = np.reshape(np_good, (np_good.shape[0], 1))\n",
    "    return good_reshaped, area_reshaped\n",
    "\n",
    "\n",
    "# Load model and weight\n",
    "def Main(df):\n",
    "    model = load_model(\"./org_model_architecture.json\")\n",
    "    df = preproces_df(df)\n",
    "    print('predicting...')\n",
    "    MAX_SEQUENCE_LENGTH = 63 #training file 44\n",
    "    MAX_WORDS = 2500 #2000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "\n",
    "    X_test=[df['cleaned'].iloc[-1]]\n",
    "    X_test=tokenizer.texts_to_sequences(X_test)\n",
    "    X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(X_test)\n",
    "    org = ['public work (โยธา)', 'municipal office (เทศกิจ)' ,'police department']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', org[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking from ori excel\n",
    "    #print('real cat is : ',df['org'].iloc[-1])\n",
    "    if 'org' not in df.index:\n",
    "        df.loc['org'] = 0 \n",
    "\n",
    "    df['org'].iloc[-1] = org[int(predictions.argmax(axis=-1))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def Main_relevant(df):\n",
    "    model = load_model(\"./relevant_model_architecture.json\")\n",
    "    df = preproces_df(df) #df['tweet_text'] เป็นstring ของ tweet \n",
    "    print('predicting...')\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 132 #new\n",
    "    MAX_WORDS = 3500 #new\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "    word_index = tokenizer.word_index #new\n",
    "    X = tokenizer.texts_to_sequences(df.cleaned.values) #new\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) #new\n",
    "\n",
    "    good_reshaped, area_reshaped = pau_anal(df) #new - adding pau good, area\n",
    "\n",
    "    # Use np.hstack to append new column to the right\n",
    "    X = np.hstack((X, good_reshaped)) #new\n",
    "    X = np.hstack((X, area_reshaped)) #new\n",
    "    X_test = X #new\n",
    "\n",
    "    #X_test=[df['cleaned'].iloc[-1]] #new- delete\n",
    "    #X_test=tokenizer.texts_to_sequences(X_test) #new- delete\n",
    "    #X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH) #new- delete\n",
    "    predictions = model.predict(X_test)\n",
    "    rev = ['not relevant','relevant']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', rev[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking\n",
    "    print('real cat is : ',df['rev'].iloc[-1])\n",
    "\n",
    "    df_drop = df #if not change then it is relevant\n",
    "\n",
    "    if int(predictions.argmax(axis=-1))== 0:\n",
    "        print('not relevant')\n",
    "        df_drop = df[-1] #store the dropped df\n",
    "        df = df[:-1] #drop the last one\n",
    "    return df, df_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "result: [[0.11716956 0.44871703 0.43411338]]\n",
      "classs predicted:  municipal office (เทศกิจ)\n",
      "confidence %:  [0.44871703]\n",
      "real cat is :  nan\n"
     ]
    }
   ],
   "source": [
    "df_updated, df_drop = Main_relevant(\"./TestNLP_labeled_lstm.csv\") #input the df location\n",
    "if df_drop[-1] == df_updated[-1]: #relevant\n",
    "    df = Main(df_updated)\n",
    "    print('relevant, updated')\n",
    "    print('org is : ', )\n",
    "else: \n",
    "    print('irrelevant, row deleted')\n",
    "    print('deleted row : ', df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                              1004\n",
       "username                                                    AitKanphong\n",
       "tweet_id                                            1701505503111819503\n",
       "tweet_text            เหล่าสิงห์มอเตอร์ไซด์สายเท่ทุกท่านครับ กทม จับ...\n",
       "org                                                                 NaN\n",
       "relevant                                                          False\n",
       "category                                                        ทางเท้า\n",
       "query                 (\"ทางเท้า\" OR \"ทางเดิน\" OR \"ฟุตบาท\") AND (\"กรุ...\n",
       "datetime_of_tweet                                    2023-09-12 7:58:19\n",
       "datetime_of_query                                   2023-11-16 14:52:12\n",
       "link                  https://twitter.com/AitKanphong/status/1701505...\n",
       "mentioned_location                                              มีนบุรี\n",
       "location                                                            NaN\n",
       "image                 ['https://pbs.twimg.com/media/F5z1289bgAAzKC0?...\n",
       "split_text            [เหล่า, สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท...\n",
       "combined              เหล่า สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน ครับ...\n",
       "cleaned               สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน กทม อ. เอ ...\n",
       "split_cleaned         [สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท่าน, กท...\n",
       "word_length                                                          38\n",
       "Name: 552, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "X_test=tokenizer.texts_to_sequences(X_test)\n",
    "X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
